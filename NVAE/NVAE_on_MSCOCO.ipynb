{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE on MSCOCO.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYX7blxLf2GX"
      },
      "source": [
        "!wget http://images.cocodataset.org/zips/train2014.zip\n",
        "!unzip /content/train2014.zip -d /content\n",
        "!rm /content/train2014.zip\n",
        "!wget http://images.cocodataset.org/zips/test2014.zip\n",
        "!unzip /content/test2014.zip -d /content\n",
        "!rm /content/test2014.zip\n",
        "!wget http://images.cocodataset.org/zips/val2014.zip\n",
        "!unzip /content/val2014.zip -d /content\n",
        "!rm /content/val2014.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pkqv7zFqPqle"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "from os import listdir\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNTuCz-jP5yC"
      },
      "source": [
        "class CocoDataloader(object):\n",
        "\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.transform = transform\n",
        "        self.image_names = [os.path.join(data_dir, img) for img in listdir(data_dir) if os.path.join(data_dir, img)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image = Image.open(self.image_names[idx])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIEpPnKkQxxK"
      },
      "source": [
        "coco_train_data = '/content/train2014'\n",
        "coco_valid_data = '/content/val2014'\n",
        "coco_test_data = '/content/test2014'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XvfDAPucciP"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy\n",
        "import json\n",
        "import torch\n",
        "import logging\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim.lr_scheduler import ExponentialLR"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTyX_FKfgA4y",
        "outputId": "6c712f51-f489-430b-8ec9-5887bee67efa"
      },
      "source": [
        "selectedDevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Used device: {selectedDevice}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMKHgtmT5RQk"
      },
      "source": [
        "image_crop = 375\n",
        "image_size = 64\n",
        "mean = [0.5, 0.5, 0.5]\n",
        "std = [0.5, 0.5, 0.5]\n",
        "batch_size = 256\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUyvzNSviLiC"
      },
      "source": [
        "training_data = CocoDataloader(coco_train_data,\n",
        "                                transform=transforms.Compose([transforms.CenterCrop((image_crop, image_crop)),\n",
        "                                                              transforms.Resize((image_size, image_size)),\n",
        "                                                              transforms.RandomHorizontalFlip(),\n",
        "                                                              transforms.ToTensor(),\n",
        "                                                              GreyToColor(image_size),\n",
        "                                                              transforms.Normalize(mean, std)\n",
        "                                                              ]))\n",
        "validation_data = CocoDataloader(coco_valid_data,\n",
        "                                  transform=transforms.Compose([transforms.CenterCrop((image_crop, image_crop)),\n",
        "                                                                transforms.Resize((image_size, image_size)),\n",
        "                                                                transforms.ToTensor(),\n",
        "                                                                GreyToColor(image_size),\n",
        "                                                                transforms.Normalize(mean, std)\n",
        "                                                                ]))\n",
        "test_data = CocoDataloader(coco_test_data,\n",
        "                            transform=transforms.Compose([transforms.CenterCrop((image_crop, image_crop)),\n",
        "                                                          transforms.Resize((image_size, image_size)),\n",
        "                                                          transforms.RandomHorizontalFlip(),\n",
        "                                                          transforms.ToTensor(),\n",
        "                                                          GreyToColor(image_size),\n",
        "                                                          transforms.Normalize(mean, std)\n",
        "                                                          ]))\n",
        "\n",
        "train_test_data = ConcatDataset([training_data, test_data])\n",
        "\n",
        "dataloader_train = DataLoader(train_test_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "dataloader_valid = DataLoader(validation_data, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfQqdmr35AvC"
      },
      "source": [
        "class depthwise_separable_conv(nn.Module):\n",
        "    def __init__(self, nin, kernels_per_layer, nout):\n",
        "        super(depthwise_separable_conv, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(nin, nin * kernels_per_layer, kernel_size=5, padding=2, groups=nin)\n",
        "        self.pointwise = nn.Conv2d(nin * kernels_per_layer, nout, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.depthwise(x)\n",
        "        out = self.pointwise(out)\n",
        "        return out"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afjSdvr859aY"
      },
      "source": [
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkNYn0nn5_a8"
      },
      "source": [
        "class ChannelSELayer(nn.Module):\n",
        "    def __init__(self, num_channels, reduction_ratio=2):\n",
        "        super(ChannelSELayer, self).__init__()\n",
        "        num_channels_reduced = num_channels // reduction_ratio\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "        self.fc1 = nn.Linear(num_channels, num_channels_reduced, bias=True)\n",
        "        self.fc2 = nn.Linear(num_channels_reduced, num_channels, bias=True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        batch_size, num_channels, H, W = input_tensor.size()\n",
        "        squeeze_tensor = input_tensor.view(batch_size, num_channels, -1).mean(dim=2)\n",
        "\n",
        "        fc_out_1 = self.relu(self.fc1(squeeze_tensor))\n",
        "        fc_out_2 = self.sigmoid(self.fc2(fc_out_1))\n",
        "\n",
        "        a, b = squeeze_tensor.size()\n",
        "        output_tensor = torch.mul(input_tensor, fc_out_2.view(a, b, 1, 1))\n",
        "        return output_tensor"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al44zRp16CqT"
      },
      "source": [
        "class dec_res(nn.Module):\n",
        "  def __init__(self,in_channel):\n",
        "    super(dec_res,self).__init__()\n",
        "    self.bn1 = nn.BatchNorm2d(in_channel)\n",
        "    self.c1 = nn.Conv2d(in_channels=in_channel,out_channels=2*in_channel,kernel_size=1,stride=1,padding=0)\n",
        "    self.bn2 = nn.BatchNorm2d(2*in_channel)\n",
        "    self.dc1 = depthwise_separable_conv(nin=2*in_channel,kernels_per_layer=3,nout=2*in_channel)\n",
        "    self.bn3 = nn.BatchNorm2d(2*in_channel)\n",
        "    self.c2 = nn.Conv2d(in_channels=2*in_channel,out_channels=in_channel,kernel_size=1,stride=1,padding=0)\n",
        "    self.bn4 = nn.BatchNorm2d(in_channel)\n",
        "    self.SE = ChannelSELayer(in_channel)\n",
        "  def forward(self,x1):\n",
        "    x = self.c1(self.bn1(x1))\n",
        "    x = swish(self.bn2(x))\n",
        "    x = self.dc1(x)\n",
        "    x = swish(self.bn3(x))\n",
        "    x = self.bn4(self.c2(x))\n",
        "    x = self.SE(x)\n",
        "    return x+x1"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkUKc38t6ESf"
      },
      "source": [
        "class enc_res(nn.Module):\n",
        "  def __init__(self,in_channel):\n",
        "    super(enc_res,self).__init__()\n",
        "    self.bn1 = nn.BatchNorm2d(in_channel)\n",
        "    self.c1 = nn.Conv2d(in_channels=in_channel,out_channels=2*in_channel,kernel_size=3,stride=1,padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(2*in_channel)\n",
        "    self.c2 = nn.Conv2d(in_channels=2*in_channel,out_channels=in_channel,kernel_size=3,stride=1,padding=1)\n",
        "    self.bn3 = nn.BatchNorm2d(in_channel)\n",
        "    self.SE = ChannelSELayer(in_channel)\n",
        "  def forward(self,x1):\n",
        "    x = self.c1(swish(self.bn1(x1)))\n",
        "    x = self.c2(swish(self.bn2(x)))\n",
        "    x = self.SE(x)\n",
        "    return x+x1"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUVc1SN16Fms"
      },
      "source": [
        "class NVAE(nn.Module):\n",
        "  def __init__(self,start_channel,original_dim):\n",
        "    super(NVAE,self).__init__()\n",
        "    self.original_dim = original_dim\n",
        "    self.conv1 = nn.Conv2d(in_channels=start_channel,out_channels=8,kernel_size=3,stride=1,padding=1)\n",
        "    self.encblock1 = enc_res(8)\n",
        "    self.dsconv1 = nn.Conv2d(in_channels=8,out_channels=8,kernel_size=2,stride=2,padding=0)\n",
        "    self.encblock2 = enc_res(8)\n",
        "    self.dsconv2 = nn.Conv2d(in_channels=8,out_channels=8,kernel_size=2,stride=2,padding=0)\n",
        "\n",
        "    self.qmu1 = nn.Linear(original_dim*original_dim*2,original_dim*original_dim*2)\n",
        "    self.qvar1 = nn.Linear(original_dim*original_dim*2,original_dim*original_dim*2)\n",
        "    \n",
        "    self.qmu0 = nn.Linear(original_dim*original_dim//2,original_dim*original_dim//2)\n",
        "    self.qvar0 = nn.Linear(original_dim*original_dim//2,original_dim*original_dim//2)\n",
        "\n",
        "    self.pmu1 = nn.Linear(original_dim*original_dim*2,original_dim*original_dim*2)\n",
        "    self.pvar1 = nn.Linear(original_dim*original_dim*2,original_dim*original_dim*2)\n",
        "\n",
        "    self.decblock1 = dec_res(8)\n",
        "    self.usconv1 = nn.ConvTranspose2d(in_channels=8,out_channels=8,kernel_size=2,stride=2,padding=0)\n",
        "    self.decblock2 = dec_res(16)\n",
        "    self.usconv2 = nn.ConvTranspose2d(in_channels=16,out_channels=16,kernel_size=2,stride=2,padding=0)\n",
        "    self.decblock3 = dec_res(16)\n",
        "    self.finconv = nn.Conv2d(in_channels=16,out_channels=start_channel,kernel_size=3,stride=1,padding=1)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    z1 = self.dsconv1(self.encblock1(self.conv1(x)))\n",
        "    z0 = self.dsconv2(self.encblock2(z1))\n",
        "\n",
        "    qmu0 = self.qmu0(z0.reshape(z0.shape[0],self.original_dim*self.original_dim//2))\n",
        "    qvar0 = self.qvar0(z0.reshape(z0.shape[0],self.original_dim*self.original_dim//2))\n",
        "\n",
        "    qmu1 = self.qmu1(z1.reshape(z1.shape[0],self.original_dim*self.original_dim*2))\n",
        "    qvar1 = self.qvar1(z1.reshape(z1.shape[0],self.original_dim*self.original_dim*2))\n",
        "\n",
        "    stdvar0 = qvar0.mul(0.5).exp_()\n",
        "    stdvar1 = qvar1.mul(0.5).exp_()\n",
        "\n",
        "    e0 = torch.randn(qmu0.shape).to(device)\n",
        "    ez0 = qmu0+e0*stdvar0\n",
        "    ez0 = ez0.reshape(ez0.shape[0],8,self.original_dim//4,self.original_dim//4)\n",
        "    ez1 = self.usconv1(self.decblock1(ez0))\n",
        "\n",
        "    pmu1 = self.pmu1(ez1.reshape(ez1.shape[0],self.original_dim*self.original_dim*2))\n",
        "    pvar1 = self.pvar1(ez1.reshape(ez1.shape[0],self.original_dim*self.original_dim*2))\n",
        "\n",
        "    pstdvar1 = pvar1.mul(0.5).exp_()\n",
        "\n",
        "    e2 = torch.randn(qmu1.shape).to(device)\n",
        "    ez2 = pmu1+qmu1 + e2*pstdvar1*stdvar1\n",
        "    ez2 = ez2.reshape(ez2.shape[0],8,self.original_dim//2,self.original_dim//2)\n",
        "    \n",
        "    final = torch.cat((ez1,ez2),1)\n",
        "\n",
        "    recons = nn.Sigmoid()(self.finconv(self.decblock3(self.usconv2(self.decblock2(final)))))\n",
        "\n",
        "    return qmu0,qvar0,qmu1,qvar1,pmu1,pvar1,recons\n",
        "\n",
        "  def sample(self,bs):\n",
        "    e = torch.randn([bs,8,self.original_dim//4,self.original_dim//4]).to(device)\n",
        "    ez1 = self.usconv1(self.decblock1(e))\n",
        "\n",
        "    pmu1 = self.pmu1(ez1.reshape(ez1.shape[0],self.original_dim*self.original_dim*2))\n",
        "    pvar1 = self.pvar1(ez1.reshape(ez1.shape[0],self.original_dim*self.original_dim*2))\n",
        "\n",
        "    stdvar1 = pvar1.mul(0.5).exp_()\n",
        "\n",
        "    e1 = torch.randn([ez1.shape[0],self.original_dim*self.original_dim*2]).to(device)\n",
        "    e1 = pmu1 + e1*stdvar1\n",
        "    e1 = e1.reshape(e1.shape[0],8,self.original_dim//2,self.original_dim//2)\n",
        "    recons = nn.Sigmoid()(self.finconv(self.decblock3(self.usconv2(self.decblock2(torch.cat((ez1,e1),1))))))\n",
        "\n",
        "    return recons\n",
        "\n",
        "  def loss(self,x):\n",
        "    qmu0,qvar0,qmu1,qvar1,pmu1,pvar1,recons = self.forward(x)\n",
        "    klz0 = 0.5*torch.sum(torch.square(qmu0)+qvar0.exp()-qvar0-1)/x.shape[0]\n",
        "    klz1 = 0.5*torch.sum(torch.square(qmu1)/pvar1.exp()+qvar1.exp()-qvar1-1)\n",
        "    reconsloss = nn.BCELoss()(recons,x)\n",
        "    return klz0,klz1,reconsloss"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6_crrPv6I_q"
      },
      "source": [
        "model = NVAE(3,64).to(selectedDevice)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF4TNg9h7FAS"
      },
      "source": [
        "optim = torch.optim.Adamax(model.parameters())\n",
        "device = selectedDevice"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sHCdMNy75h7"
      },
      "source": [
        "epochs=20"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z74fqDN_8nm_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "HvYddKjY9VFA",
        "outputId": "7bb170c7-1be5-4e31-858e-de8bd6ec209c"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    minloss = 1\n",
        "    running_kl0_loss=0\n",
        "    running_recons_loss=0\n",
        "    running_kl1_loss=0\n",
        "    num_images=0\n",
        "    for i, img in enumerate(dataloader_train):\n",
        "      img = img.to(selectedDevice)\n",
        "      optim.zero_grad()\n",
        "      klz0,klz1,recons = model.loss(img)\n",
        "      loss=recons+epoch*0.0001*klz0+epoch*0.0001*klz1\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "      running_kl0_loss = running_kl0_loss + klz0.item()*len(img)\n",
        "      running_kl1_loss = running_kl1_loss + klz1.item()*len(img)\n",
        "      running_recons_loss = running_recons_loss + recons.item()*len(img)\n",
        "\n",
        "      num_images= num_images+len(img)\n",
        "    print('epoch: '+str(epoch)+' kl0_loss: '+str(running_kl0_loss/num_images)+' recons_loss: '+str(running_recons_loss/num_images)+' kl1_loss: '+str(running_kl1_loss/num_images))\n",
        "    imgs = model.sample(64).cpu().detach().reshape(64,28,28)\n",
        "    plt.gray()\n",
        "    fig = plt.figure(figsize=(8., 8.))\n",
        "    grid = ImageGrid(fig, 111, nrows_ncols=(8, 8), axes_pad=0.05)\n",
        "\n",
        "    for ax, im in zip(grid, imgs):\n",
        "        ax.imshow(im)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-a501f81f0bd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecons\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mklz0\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mklz1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0mrunning_kl0_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_kl0_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mklz0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mrunning_kl1_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_kl1_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mklz1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adamax.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     94\u001b[0m                      \u001b[0mbeta2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                      \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                      weight_decay=weight_decay)\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madamax\u001b[0;34m(params, grads, exp_avgs, exp_infs, state_steps, eps, beta1, beta2, lr, weight_decay)\u001b[0m\n\u001b[1;32m    315\u001b[0m         norm_buf = torch.cat([\n\u001b[1;32m    316\u001b[0m             \u001b[0mexp_inf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         ], 0)\n\u001b[1;32m    319\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp_inf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W2M8iR9BOAf",
        "outputId": "a54f025a-93ef-48e0-ec40-2853c4377862"
      },
      "source": [
        "image.size"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(640, 425)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtQZTOgQBOrd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}